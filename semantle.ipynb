{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from gensim.models import KeyedVectors\n",
    "import random \n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.spatial import distance\n",
    "import keyboard\n",
    "\n",
    "def get_vector(model, word):\n",
    "    '''\n",
    "    Model must be gensim object Word2Vec model\n",
    "    Returns word vector if key present in its vocab\n",
    "    '''\n",
    "    try:\n",
    "        vect = model[word]\n",
    "    except:\n",
    "        vect = None\n",
    "        print(\"Word not in model vocabulary\")\n",
    "    return vect\n",
    "\n",
    "def vec2word(model, vectors, top_n=1):\n",
    "    '''\n",
    "    Find most similar word in model given vector\n",
    "    '''\n",
    "    if isinstance(vectors, float):\n",
    "        vectors = [vectors]\n",
    "\n",
    "    # Find the word most similar to the given vector\n",
    "    ave_vector = np.mean(vectors, axis=0)\n",
    "    most_similar_words = model.similar_by_vector(ave_vector, topn=top_n+len(vectors))\n",
    "    most_similar_words = [word for word, _ in most_similar_words]\n",
    "\n",
    "    return most_similar_words[len(vectors):]\n",
    "\n",
    "def cossim(model, vocab, word_1, word_2):\n",
    "    # make sure both words in Word2Vec model\n",
    "    if word_1 in vocab and word_2 in vocab:\n",
    "        return (1 - distance.cosine(model[word_1], model[word_2])) \n",
    "    else:\n",
    "        print(\"At least one word not in model vocab\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def n_most_similar_words(model, vocab, words, neg=None, n=10):\n",
    "    '''\n",
    "    negative is a list of words opposite of most similar n words\n",
    "    '''\n",
    "\n",
    "    if isinstance(words, str):\n",
    "        words = [words]\n",
    "\n",
    "    if (neg is None) and all(w in vocab for w in words):\n",
    "        return model.most_similar(words, topn=n)\n",
    "    elif (words is None) and all(n in vocab for n in neg):\n",
    "        return model.most_similar(negative=neg, topn=n)\n",
    "    elif all(w in vocab for w in words) and all(n in vocab for n in neg):\n",
    "        return model.most_similar(positive=words, negative=neg, topn=n)\n",
    "    else:\n",
    "        print(\"Words not in model vocabulary\")\n",
    "        return None\n",
    "    \n",
    "def skip_gram(model, vocab, context_words, n):\n",
    "    # Predict the most similar n words\n",
    "    \n",
    "    if isinstance(context_words, str):\n",
    "        context_words = [context_words]\n",
    "\n",
    "    if all(w in vocab for w in context_words):\n",
    "        context_vectors = [model[word] for word in context_words]\n",
    "        avg_vector = np.mean(context_vectors, axis=0)\n",
    "        similar_words = model.similar_by_vector(avg_vector, topn=n+len(context_words))\n",
    "        return similar_words[len(context_words):]\n",
    "\n",
    "    else:\n",
    "        print(\"Words not in model vocabulary\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/77/br1gng9x2hlgs5t_9z9pwb3w0000gn/T/ipykernel_73031/2703000182.py:13: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True) #Precompute L2-normalized vectors. If replace is set to TRUE, forget the original vectors and only keep the normalized ones. Saves lots of memory, but can't continue to train the model.\n"
     ]
    }
   ],
   "source": [
    "#your \"sentences\" object with the cleaned text data. \n",
    "\n",
    "#bigram_transformer = phrases.Phrases(docs)\n",
    "#bigram = phrases.Phraser(bigram_transformer)\n",
    "\n",
    "# model = Word2Vec(bigram[docs], workers=4, sg=0, min_count=5, window=5, sample=1e-3) #size=700 # N-dimensions\n",
    "# model =  Word2Vec.load(\"./Word2Vec_Models/English_Sample\") #name of YOUR model here, or file path to your \n",
    "\n",
    "# Load Google News Word2Vec Model\n",
    "\n",
    "model_path = './Word2Vec_Models/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, limit=500000, binary=True)\n",
    "model.init_sims(replace=True) #Precompute L2-normalized vectors. If replace is set to TRUE, forget the original vectors and only keep the normalized ones. Saves lots of memory, but can't continue to train the model.\n",
    "#vocab = list(model.index_to_key)\n",
    "vocab = [word for word in model.index_to_key if re.match(\"^[a-zA-Z.-]+$\", word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Complete\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Word2Vec_Models/eng_vocab.txt\", \"w\") as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + \"\\n\")\n",
    "print(\"Model Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love Barington\n",
      "[('love', 1.0), ('loved', 0.6907792091369629)]\n",
      "['loved']\n",
      "-0.01563189923763275\n",
      "[('loved', 0.6907792091369629), ('adore', 0.6816874146461487), ('loves', 0.661863386631012), ('passion', 0.6100709438323975), ('hate', 0.600395679473877)]\n",
      "[('loved', 0.6907792091369629), ('adore', 0.6816874146461487), ('loves', 0.661863386631012), ('passion', 0.6100709438323975), ('hate', 0.600395679473877)]\n"
     ]
    }
   ],
   "source": [
    "# helper function usage\n",
    "\n",
    "guess, target = \"love\", random.choice(vocab)\n",
    "print(guess, target)\n",
    "\n",
    "guess_vect = get_vector(model, guess)\n",
    "#print(guess_vect)\n",
    "\n",
    "word = vec2word(model, [guess_vect])\n",
    "print(word)\n",
    "\n",
    "score = cossim(model, vocab, guess, target)\n",
    "print(score)\n",
    "\n",
    "top_n = n_most_similar_words(model, vocab, [guess], None, 5)\n",
    "print(top_n)\n",
    "\n",
    "context = skip_gram(model, vocab, [guess], 5)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  1\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  2\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  3\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  4\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  5\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  6\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  7\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  8\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  9\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  10\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  11\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  12\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  13\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  14\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  15\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  16\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  17\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  18\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  19\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  20\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  21\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  22\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  23\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  24\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  25\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  26\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  27\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  28\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  29\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  30\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  31\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  32\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  33\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  34\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  35\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  36\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  37\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  38\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  39\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  40\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  41\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  42\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  43\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  44\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  45\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  46\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  47\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  48\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  49\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  50\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  51\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  52\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  53\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  54\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  55\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  56\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  57\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  58\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  59\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  60\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  61\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  62\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  63\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  64\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  65\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  66\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  67\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  68\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  69\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  70\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  71\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  72\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  73\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  74\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  75\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  76\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  77\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  78\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  79\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  80\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  81\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  82\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  83\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  84\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  85\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  86\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  87\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  88\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  89\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  90\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  91\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  92\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  93\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  94\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  95\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  96\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  97\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  98\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  99\n",
      "Similarity Score:  -1.71\n",
      "Guess:  loved\n",
      "Turns:  100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mabs\u001b[39m(score) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m delta:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     turns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     guess \u001b[39m=\u001b[39m vec2word(model, [guess_vect])[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     score \u001b[39m=\u001b[39m cossim(model, vocab, guess, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     scores\u001b[39m.\u001b[39mappend(score)\n",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb Cell 9\u001b[0m line \u001b[0;36mvec2word\u001b[0;34m(model, vectors, top_n)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Find the word most similar to the given vector\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m ave_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m most_similar_words \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49msimilar_by_vector(ave_vector, topn\u001b[39m=\u001b[39;49mtop_n\u001b[39m+\u001b[39;49m\u001b[39mlen\u001b[39;49m(vectors))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m most_similar_words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word, _ \u001b[39min\u001b[39;00m most_similar_words]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m most_similar_words[\u001b[39mlen\u001b[39m(vectors):]\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:914\u001b[0m, in \u001b[0;36mKeyedVectors.similar_by_vector\u001b[0;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilar_by_vector\u001b[39m(\u001b[39mself\u001b[39m, vector, topn\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, restrict_vocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    891\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the top-N most similar keys by vector.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \n\u001b[1;32m    893\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \n\u001b[1;32m    913\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 914\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmost_similar(positive\u001b[39m=\u001b[39;49m[vector], topn\u001b[39m=\u001b[39;49mtopn, restrict_vocab\u001b[39m=\u001b[39;49mrestrict_vocab)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:849\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n\u001b[1;32m    847\u001b[0m     \u001b[39mreturn\u001b[39;00m indexer\u001b[39m.\u001b[39mmost_similar(mean, topn)\n\u001b[0;32m--> 849\u001b[0m dists \u001b[39m=\u001b[39m dot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectors[clip_start:clip_end], mean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms[clip_start:clip_end]\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m topn:\n\u001b[1;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m dists\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iterate until guess close enough\n",
    "\n",
    "delta = 0.05\n",
    "score = 0\n",
    "turns = 0\n",
    "\n",
    "scores = []\n",
    "\n",
    "\n",
    "while abs(score) < 1 - delta:\n",
    "    turns += 1\n",
    "    guess = vec2word(model, [guess_vect])[0]\n",
    "    score = cossim(model, vocab, guess, target)\n",
    "    scores.append(score)\n",
    "    \n",
    "    # keyboard.wait('space')\n",
    "    guess_vect = guess_vect + guess_vect/2 # update guess according to complex hueristic \n",
    "\n",
    "    print(\"Turn: \", turns)\n",
    "    print(\"Guess: \", guess)\n",
    "    print(\"Similarity Score: \", round(score * 100, 2))\n",
    "    print()\n",
    "\n",
    "print(\"Close Enough! Target Word: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Text Database Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "# Your text message\n",
    "text_message = \"This is an example text message.\"\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode([text_message])\n",
    "\n",
    "# Print the embeddings\n",
    "print(\"Embeddings:\", embeddings)\n",
    "\n",
    "# If you want to convert the embeddings to a numpy array\n",
    "embeddings_np = np.array(embeddings)\n",
    "print(\"Embeddings as numpy array:\", embeddings_np)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
