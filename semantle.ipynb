{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "import random \n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.spatial import distance\n",
    "import keyboard\n",
    "\n",
    "def get_vector(model, word):\n",
    "    '''\n",
    "    Model must be gensim object Word2Vec model\n",
    "    Returns word vector if key present in its vocab\n",
    "    '''\n",
    "    try:\n",
    "        vect = model[word]\n",
    "    except:\n",
    "        vect = None\n",
    "        print(\"Word not in model vocabulary\")\n",
    "    return vect\n",
    "\n",
    "def vec2word(model, vectors, top_n=1):\n",
    "    '''\n",
    "    Find most similar words as list in model given vector\n",
    "    '''\n",
    "    if isinstance(vectors, float):\n",
    "        vectors = [vectors]\n",
    "\n",
    "    # Find the word most similar to the given vector\n",
    "    ave_vector = np.mean(vectors, axis=0)\n",
    "    most_similar_words = model.similar_by_vector(ave_vector, topn=top_n+len(vectors))\n",
    "    most_similar_words = [word for word, _ in most_similar_words]\n",
    "\n",
    "    return most_similar_words[len(vectors):]\n",
    "\n",
    "def cossim(model, vocab, word_1, word_2):\n",
    "    # make sure both words in Word2Vec model\n",
    "    if word_1 in vocab and word_2 in vocab:\n",
    "        return (1 - distance.cosine(model[word_1], model[word_2])) \n",
    "    else:\n",
    "        print(\"At least one word not in model vocab\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def n_most_similar_words(model, vocab, words, neg=None, n=10):\n",
    "    '''\n",
    "    negative is a list of words opposite of most similar n words\n",
    "    '''\n",
    "\n",
    "    if isinstance(words, str):\n",
    "        words = [words]\n",
    "\n",
    "    if (neg is None) and all(w in vocab for w in words):\n",
    "        return model.most_similar(words, topn=n)\n",
    "    elif (words is None) and all(n in vocab for n in neg):\n",
    "        return model.most_similar(negative=neg, topn=n)\n",
    "    elif all(w in vocab for w in words) and all(n in vocab for n in neg):\n",
    "        return model.most_similar(positive=words, negative=neg, topn=n)\n",
    "    else:\n",
    "        print(\"Words not in model vocabulary\")\n",
    "        return None\n",
    "    \n",
    "def skip_gram(model, vocab, context_words, n=10):\n",
    "    # Predict the most similar n words\n",
    "    \n",
    "    if isinstance(context_words, str):\n",
    "        context_words = [context_words]\n",
    "\n",
    "    if all(w in vocab for w in context_words):\n",
    "        context_vectors = [model[word] for word in context_words]\n",
    "        avg_vector = np.mean(context_vectors, axis=0)\n",
    "        similar_words = model.similar_by_vector(avg_vector, topn=n+len(context_words))\n",
    "        return similar_words[len(context_words):]\n",
    "\n",
    "    else:\n",
    "        print(\"Words not in model vocabulary\")\n",
    "        print(\"Not found:\", context_words)\n",
    "        return None\n",
    "    \n",
    "def normalize(v):\n",
    "    '''\n",
    "    Normalize a vector to unit magnitude\n",
    "    '''\n",
    "    v_norm = v / np.linalg.norm(v)\n",
    "    #v_norm = v_norm / np.linalg.norm(v_norm)\n",
    "    return v_norm\n",
    "\n",
    "def get_orthogonal(model, word, max_return=5):\n",
    "    '''\n",
    "    Return N words from vectors orthogonal to word vector by finding projections on hyperplane\n",
    "    '''\n",
    "    v = get_vector(model, word)\n",
    "    n = len(v)\n",
    "    words = []\n",
    "    valid = []\n",
    "\n",
    "    # Create an identity matrix of size 300x300\n",
    "    identity_matrix = np.eye(n)\n",
    "\n",
    "    # Subtract the projection of v onto each standard basis vector\n",
    "    # to obtain orthogonal vectors on the hyperplane perpendicular to v\n",
    "    orthogonal_matrix = np.array([identity_matrix[i] - np.dot(v, identity_matrix[i]) / np.dot(v, v) * v for i in range(n)])\n",
    "\n",
    "    # add valid words from orthogonal matrix\n",
    "    count = 0\n",
    "    for index, v in enumerate(orthogonal_matrix):\n",
    "        #if count == max_return:\n",
    "        #    break\n",
    "        w = vec2word(model, [normalize(v)])\n",
    "        if re.match(\"^[a-z]+$\", w[0]):\n",
    "            valid.append(w[0])\n",
    "            #words.append(w)\n",
    "            count += 1\n",
    "    \n",
    "    #orthogonal_vectors = orthogonal_matrix[:max_return]\n",
    "    #normalized_vectors = [normalize(v) for v in orthogonal_vectors] # orthogonal_vectors / magnitude(orthogonal_vectors, axis=1)[:, np.newaxis]\n",
    "    return np.random.choice(valid, 10, replace=False) #words # normalized_vectors\n",
    "\n",
    "def magnitude(vector):\n",
    "    '''\n",
    "    Compute magnitude of vector\n",
    "    '''\n",
    "    return np.linalg.norm(vector)\n",
    "\n",
    "def replace(model, words):\n",
    "    '''\n",
    "    words: tuple of words, 3rd word replaces concept of 2nd word in the context of 1st word\n",
    "    '''\n",
    "    vects = [get_vector(model, w) for w in words]\n",
    "    v1, v2, v3 = vects\n",
    "    new_vec = v1 - v2 + v3\n",
    "\n",
    "    return vec2word(model, [new_vec])\n",
    "\n",
    "def interpolate(model, vocab, words, scores):\n",
    "    vects = [get_vector(model, word) for word in words]\n",
    "    n = len(vects[0])\n",
    "    ave_vect = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(len(vects)):\n",
    "            ave_vect[i] += scores[j] * vects[j][i]\n",
    "\n",
    "    ave_vect = ave_vect / np.linalg.norm(ave_vect)\n",
    "    ave_word = vec2word(model, [ave_vect])\n",
    "    ave_words = skip_gram(model, vocab, ave_word, 10)\n",
    "    \n",
    "    return ave_vect, ave_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "model = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "vocab = [word for word in model.index_to_key if re.match(\"^[a-zA-Z.-]+$\", word)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['girl']\n",
      "At least one word not in model vocab\n",
      "None\n",
      "[('girl', 0.7885462045669556), ('teenager', 0.7737529277801514), ('woman', 0.743568480014801), ('teenage_girl', 0.7086503505706787), ('teen_ager', 0.6568499207496643), ('toddler', 0.6250425577163696), ('youngster', 0.5938030481338501), ('lad', 0.5809065699577332), ('kid', 0.5788402557373047), ('son', 0.577247142791748)]\n"
     ]
    }
   ],
   "source": [
    "guesses = ['man', 'boy']\n",
    "\n",
    "guess_vect = get_vector(model, guesses)\n",
    "#print(guess_vect)\n",
    "word = vec2word(model, guess_vect)\n",
    "print(word)\n",
    "\n",
    "score = cossim(model, vocab, 'man', 'girl')\n",
    "print(score)\n",
    "\n",
    "top_n = n_most_similar_words(model, vocab, guesses, None, 10)\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2551112473011017\n"
     ]
    }
   ],
   "source": [
    "score = cossim(model, vocab, 'man', 'female')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/77/br1gng9x2hlgs5t_9z9pwb3w0000gn/T/ipykernel_1268/1388857392.py:13: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True) #Precompute L2-normalized vectors. If replace is set to TRUE, forget the original vectors and only keep the normalized ones. Saves lots of memory, but can't continue to train the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Filter and clean the model\\nfiltered_index_to_key = [word for word in model.index_to_key if re.match(\"^[a-z]+$\", word)]\\nfiltered_vectors = model.vectors[[model.index_to_key.index(word) for word in filtered_index_to_key]]\\n\\n# Update the original model with the filtered version\\nmodel.index_to_key = filtered_index_to_key\\nmodel.vectors = filtered_vectors\\n\\n# Now, vocab contains only lowercase valid words\\nvocab = list(filtered_index_to_key)\\n'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your \"sentences\" object with the cleaned text data. \n",
    "\n",
    "#bigram_transformer = phrases.Phrases(docs)\n",
    "#bigram = phrases.Phraser(bigram_transformer)\n",
    "\n",
    "# model = Word2Vec(bigram[docs], workers=4, sg=0, min_count=5, window=5, sample=1e-3) #size=700 # N-dimensions\n",
    "# model =  Word2Vec.load(\"./Word2Vec_Models/English_Sample\") #name of YOUR model here, or file path to your \n",
    "\n",
    "# Load Google News Word2Vec Model\n",
    "\n",
    "model_path = './Word2Vec_Models/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, limit=500000, binary=True)\n",
    "model.init_sims(replace=True) #Precompute L2-normalized vectors. If replace is set to TRUE, forget the original vectors and only keep the normalized ones. Saves lots of memory, but can't continue to train the model.\n",
    "vocab = [word for word in model.index_to_key if re.match(\"^[a-z]+$\", word)]\n",
    "\n",
    "'''\n",
    "# Filter and clean the model\n",
    "filtered_index_to_key = [word for word in model.index_to_key if re.match(\"^[a-z]+$\", word)]\n",
    "filtered_vectors = model.vectors[[model.index_to_key.index(word) for word in filtered_index_to_key]]\n",
    "\n",
    "# Update the original model with the filtered version\n",
    "model.index_to_key = filtered_index_to_key\n",
    "model.vectors = filtered_vectors\n",
    "\n",
    "# Now, vocab contains only lowercase valid words\n",
    "vocab = list(filtered_index_to_key)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "75045\n",
      "['queen']\n",
      "0.6824870109558105\n",
      "yayy\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "word = 'dog'\n",
    "print(word in vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "words = ['king', 'man', 'woman']\n",
    "new_word = replace(model, words)\n",
    "print(new_word)\n",
    "print(cossim(model, vocab, 'man', 'boy'))\n",
    "\n",
    "w = 'ontext'\n",
    "if re.match(\"^[a-z]+$\", w):\n",
    "    print(\"yayy\")\n",
    "print(vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Load the Word2Vec model from the pickle file\\nwith open(model_path, 'rb') as file:\\n    model = pickle.load(file)\\n\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the Word2Vec model using pickle\n",
    "model_path = './Word2Vec_Models/google_word2vec.pkl'\n",
    "with open(model_path, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "'''\n",
    "# Load the Word2Vec model from the pickle file\n",
    "with open(model_path, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Complete\n"
     ]
    }
   ],
   "source": [
    "# preprocess vocab\n",
    "\n",
    "with open(\"./Word2Vec_Models/eng_vocab.txt\", \"w\") as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + \"\\n\")\n",
    "print(\"Model Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: woman Target: harpoon\n",
      "Word: ['man']\n",
      "Guess to target similarity: 0.026805367320775986\n",
      "Top K Similar Words:\n",
      " [('targets', 0.6589586734771729), ('goals', 0.6300962567329407), ('objective', 0.5616191625595093), ('aim', 0.5383525490760803), ('Goal', 0.4901687204837799), ('objectives', 0.4835728704929352), ('targeted', 0.4605602025985718), ('targeting', 0.45360010862350464), ('aiming', 0.4502216875553131), ('achievable', 0.4414016008377075)]\n",
      "[('targets', 0.6589586734771729), ('goals', 0.6300962567329407), ('objective', 0.5616191625595093), ('aim', 0.5383525490760803), ('Goal', 0.4901687204837799), ('objectives', 0.4835728704929352), ('targeted', 0.4605602025985718), ('targeting', 0.45360010862350464), ('aiming', 0.4502216875553131), ('achievable', 0.4414016008377075)]\n"
     ]
    }
   ],
   "source": [
    "# helper function usage\n",
    "\n",
    "guess, target = \"woman\", random.choice(vocab)\n",
    "guesses = ['target', 'goal']\n",
    "\n",
    "print(\"Guess:\", guess, \"Target:\", target)\n",
    "\n",
    "guess_vect = get_vector(model, guess)\n",
    "#print(guess_vect)\n",
    "\n",
    "word = vec2word(model, [guess_vect])\n",
    "print(\"Word:\", word)\n",
    "\n",
    "score = cossim(model, vocab, guess, target)\n",
    "print(\"Guess to target similarity:\", score)\n",
    "\n",
    "top_n = n_most_similar_words(model, vocab, guesses, None, 10)\n",
    "print(\"Top K Similar Words:\\n\", top_n)\n",
    "\n",
    "context = skip_gram(model, vocab, guesses, 10)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconvert: ['woman']\n",
      "condensing\n",
      "licensors\n",
      "admired\n",
      "haphazard\n",
      "substitution\n",
      "tigers\n",
      "underplaying\n",
      "pointed\n",
      "goldrush\n",
      "stinger\n",
      "Interpolate: [('dog', 0.8680490255355835), ('canines', 0.8181710243225098), ('cats', 0.76517653465271), ('pit_bulls', 0.7548302412033081), ('pets', 0.7424418330192566), ('puppies', 0.7385991811752319), ('pooches', 0.7162365913391113), ('German_shepherds', 0.707106351852417), ('animals', 0.6985694169998169), ('pit_bull', 0.6983615159988403)]\n"
     ]
    }
   ],
   "source": [
    "#print(guess_vect)\n",
    "guess_vect = get_vector(model, \"man\")\n",
    "print(\"reconvert:\", vec2word(model, [guess_vect])) #vects[0]))\n",
    "mag = magnitude(guess_vect)\n",
    "\n",
    "words = get_orthogonal(model, 'kill')\n",
    "\n",
    "for w in words:\n",
    "    print(w)\n",
    "\n",
    "context_words = ['man', 'dog']\n",
    "scores = [0.5, 0.9]\n",
    "vec, words = interpolate(model, vocab, context_words, scores)\n",
    "print(\"Interpolate:\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s = 'o1'\n",
    "print(s[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess:  loved\n",
      "Turn:  1\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  2\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  3\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  4\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  5\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  6\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  7\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  8\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  9\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  10\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  11\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  12\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  13\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  14\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  15\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  16\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  17\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  18\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  19\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  20\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  21\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  22\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  23\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  24\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  25\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  26\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  27\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  28\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  29\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  30\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  31\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  32\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  33\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  34\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  35\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  36\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  37\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  38\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  39\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  40\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  41\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  42\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  43\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  44\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  45\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  46\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  47\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  48\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  49\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  50\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  51\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  52\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  53\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  54\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  55\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  56\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  57\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  58\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  59\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  60\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  61\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  62\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  63\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  64\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  65\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  66\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  67\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  68\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  69\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  70\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  71\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  72\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  73\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  74\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  75\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  76\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  77\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  78\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  79\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  80\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  81\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  82\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  83\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  84\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  85\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  86\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  87\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  88\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  89\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  90\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  91\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  92\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  93\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n",
      "Guess:  loved\n",
      "Turn:  94\n",
      "Guess:  loved\n",
      "Similarity Score:  -1.71\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mabs\u001b[39m(score) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m delta \u001b[39mand\u001b[39;00m turns \u001b[39m<\u001b[39m max_turns:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     turns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     guess \u001b[39m=\u001b[39m vec2word(model, [guess_vect])[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGuess: \u001b[39m\u001b[39m\"\u001b[39m, guess)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     score \u001b[39m=\u001b[39m cossim(model, vocab, guess, target)\n",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb Cell 9\u001b[0m line \u001b[0;36mvec2word\u001b[0;34m(model, vectors, top_n)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Find the word most similar to the given vector\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m ave_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m most_similar_words \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49msimilar_by_vector(ave_vector, topn\u001b[39m=\u001b[39;49mtop_n\u001b[39m+\u001b[39;49m\u001b[39mlen\u001b[39;49m(vectors))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m most_similar_words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word, _ \u001b[39min\u001b[39;00m most_similar_words]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/Semantle/semantle.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m most_similar_words[\u001b[39mlen\u001b[39m(vectors):]\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:914\u001b[0m, in \u001b[0;36mKeyedVectors.similar_by_vector\u001b[0;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilar_by_vector\u001b[39m(\u001b[39mself\u001b[39m, vector, topn\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, restrict_vocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    891\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the top-N most similar keys by vector.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \n\u001b[1;32m    893\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \n\u001b[1;32m    913\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 914\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmost_similar(positive\u001b[39m=\u001b[39;49m[vector], topn\u001b[39m=\u001b[39;49mtopn, restrict_vocab\u001b[39m=\u001b[39;49mrestrict_vocab)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:849\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n\u001b[1;32m    847\u001b[0m     \u001b[39mreturn\u001b[39;00m indexer\u001b[39m.\u001b[39mmost_similar(mean, topn)\n\u001b[0;32m--> 849\u001b[0m dists \u001b[39m=\u001b[39m dot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectors[clip_start:clip_end], mean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms[clip_start:clip_end]\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m topn:\n\u001b[1;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m dists\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iterate until guess close enough\n",
    "\n",
    "delta = 0.05\n",
    "score = 0\n",
    "turns = 0\n",
    "max_turns = 100\n",
    "scores = []\n",
    "\n",
    "\n",
    "while abs(score) < 1 - delta and turns < max_turns:\n",
    "    turns += 1\n",
    "    guess = vec2word(model, [guess_vect])[0]\n",
    "    print(\"Guess: \", guess)\n",
    "    score = cossim(model, vocab, guess, target)\n",
    "    scores.append(score)\n",
    "    \n",
    "    # keyboard.wait('space')\n",
    "    guess_vect = guess_vect #+ guess_vect/2 # update guess according to complex hueristic \n",
    "\n",
    "    print(\"Turn: \", turns)\n",
    "    print(\"Guess: \", guess)\n",
    "    print(\"Similarity Score: \", round(score * 100, 2))\n",
    "    print()\n",
    "\n",
    "print(\"Close Enough! Target Word: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Text Database Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "# Your text message\n",
    "text_message = \"This is an example text message.\"\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode([text_message])\n",
    "\n",
    "# Print the embeddings\n",
    "#print(\"Embeddings:\", embeddings)\n",
    "\n",
    "# If you want to convert the embeddings to a numpy array\n",
    "#embeddings_np = np.array(embeddings)\n",
    "#print(\"Embeddings as numpy array:\", embeddings_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
